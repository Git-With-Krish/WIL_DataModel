{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9df9c44",
   "metadata": {},
   "source": [
    "###### Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b972e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values:\n",
      "{'Rain': '49.03229367272723 mm', 'Solar Exposure': '16.674000000000003 kWh/m^2', 'Temperature': '16.012500000000006 째C', 'Carbon Emissions': '1295.91 units'}\n",
      "\n",
      "Risk:\n",
      " ['Low Risk', 'Medium Risk', 'Medium Risk', 'Low Risk']\n",
      "\n",
      "Final Result:\n",
      "Low Risk!!!\n"
     ]
    }
   ],
   "source": [
    "# Importing dataset and libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel('final_vic_data.xlsx')\n",
    "\n",
    "# Dropping suburb as Postcode and suburb means the same\n",
    "data.drop(columns='suburb', inplace=True)\n",
    "data.rename(columns = {'area_ha':'Area of the land per hectares','Solar_exposure':'Solar Exposure', 'carbon_gross_emissions':'Carbon Emissions', \n",
    "                       'tc_loss_ha':'Tree Loss per hectares'}, inplace=True)\n",
    "#data.head()\n",
    "\n",
    "# Features and target variable\n",
    "features = ['Postcode', 'threshold', 'Area of the land per hectares', 'Year', 'Tree Loss per hectares']\n",
    "target_variables = ['Rain', 'Solar Exposure', 'Temperature', 'Carbon Emissions']\n",
    "\n",
    "# Split the data into features (X) and target variables (y)\n",
    "X = data[features]\n",
    "y = data[target_variables]\n",
    "\n",
    "# Splitting the data into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Regression model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for an example input\n",
    "example_input = [[3672, 75, 821143, 2023, 1]]\n",
    "predictions = rf_model.predict(example_input)\n",
    "\n",
    "# Print the predicted values\n",
    "print(\"Predicted values:\")\n",
    "result = {}\n",
    "risk = []\n",
    "for i, var in enumerate(target_variables):\n",
    "    value = predictions[0][i]\n",
    "    unit = ''\n",
    "    if var == 'Rain':\n",
    "        unit = 'mm'\n",
    "    elif var == 'Solar Exposure':\n",
    "        unit = 'kWh/m^2'\n",
    "    elif var == 'Temperature':\n",
    "        unit = '째C'\n",
    "    elif var == 'Carbon Emissions':\n",
    "        unit = 'units'\n",
    "    result[var] = f\"{value} {unit}\"\n",
    "    #print(f\"{var}: {value} {unit}\")\n",
    "\n",
    "    #value = float(result[var].split()[0])\n",
    "    # Calculating risks based on thresholds\n",
    "    # Rain\n",
    "    if var == 'Rain':\n",
    "        if (40 < float(result[var].split()[0]) <= 50):\n",
    "            risk.append('Low Risk')\n",
    "        elif (35 < float(result[var].split()[0]) <= 40 or 50 < result['Rain'] <= 55):\n",
    "            risk.append('Medium Risk')\n",
    "        else:\n",
    "            risk.append('High Risk')\n",
    "\n",
    "    # Solar Exposure\n",
    "    elif var == 'Solar Exposure':\n",
    "        if (14 < float(result[var].split()[0]) <= 16):\n",
    "            risk.append('Low Risk')\n",
    "        elif (10 < float(result[var].split()[0]) <= 14 or 16 < float(result[var].split()[0]) <= 20):\n",
    "            risk.append('Medium Risk')\n",
    "        else:\n",
    "            risk.append('High Risk')\n",
    "\n",
    "    # Temperature\n",
    "    elif var == 'Temperature':\n",
    "        if (18 < float(result[var].split()[0]) <= 24):\n",
    "            risk.append('Low Risk')\n",
    "        elif (10 < float(result[var].split()[0]) <= 18 or 24 < float(result[var].split()[0]) <= 30):\n",
    "            risk.append('Medium Risk')\n",
    "        else:\n",
    "            risk.append('High Risk')\n",
    "\n",
    "    # Carbon Emissions\n",
    "    elif var == 'Carbon Emissions':\n",
    "        if (0 < float(result[var].split()[0]) <= 90000):\n",
    "            risk.append('Low Risk')\n",
    "        elif (90000 < float(result[var].split()[0]) <= 100000):\n",
    "            risk.append('Medium Risk')\n",
    "        else:\n",
    "            risk.append('High Risk')\n",
    "\n",
    "# Printthe results\n",
    "print(result)\n",
    "print('\\nRisk:\\n',risk)\n",
    "\n",
    "# Return the risk factor based on thresholds\n",
    "high_count=0\n",
    "low_count=0\n",
    "med_count=0\n",
    "\n",
    "for i in risk:\n",
    "    if i == 'High Risk':\n",
    "        high_count+=1\n",
    "    elif i == 'Medium Risk':\n",
    "        med_count+=1\n",
    "    else:\n",
    "        low_count+=1\n",
    "\n",
    "print('\\nFinal Result:')\n",
    "\n",
    "if high_count >= 2:\n",
    "    print('High Risk!!!')\n",
    "elif high_count == 1:\n",
    "    print('Medium Risk!!!')\n",
    "else:\n",
    "    print('Low Risk!!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dd5bb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 89885258340.52019\n",
      "Root Mean Squared Error: 299808.7029099059\n",
      "R-squared (R2) Score: -0.0922296705728145\n",
      "MAPE Score: 3.011515381337735e+19\n"
     ]
    }
   ],
   "source": [
    "                    ##### WORST SCORES....... IGNORE!!!!!!!! #####\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "import numpy as np\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared (R2) Score:\", r2)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(\"MAPE Score:\", mape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62999e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values:\n",
      "{'Rain': '50.429498178787846 mm', 'Solar Exposure': '16.669 kWh/m^2', 'Temperature': '16.043500000000005 째C', 'Carbon Emissions': '1302.43 units'}\n",
      "\n",
      "Risk:\n",
      " ['Medium Risk', 'Medium Risk', 'Medium Risk', 'Low Risk']\n",
      "\n",
      "Final Result:\n",
      "Low Risk!!!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel('final_vic_data.xlsx')\n",
    "\n",
    "# Dropping suburb as Postcode and suburb means the same\n",
    "data.drop(columns='suburb', inplace=True)\n",
    "data.rename(columns={'area_ha':'Area of the land per hectares', 'Solar_exposure':'Solar Exposure', \n",
    "                     'carbon_gross_emissions':'Carbon Emissions', 'tc_loss_ha':'Tree Loss per hectares'}, inplace=True)\n",
    "\n",
    "# Features and target variable\n",
    "features = ['Postcode', 'threshold', 'Area of the land per hectares', 'Year', 'Tree Loss per hectares']\n",
    "target_variables = ['Rain', 'Solar Exposure', 'Temperature', 'Carbon Emissions']\n",
    "\n",
    "# Split the data into features (X) and target variables (y)\n",
    "X = data[features]\n",
    "y = data[target_variables]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Splitting the data into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest Regression model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform Grid Search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from Grid Search\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Train the best model\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for an example input\n",
    "example_input = [[3672, 75, 821143, 2023, 1]]\n",
    "example_input_scaled = scaler.transform(example_input)\n",
    "predictions = best_rf_model.predict(example_input_scaled)\n",
    "\n",
    "# Print the predicted values\n",
    "print(\"Predicted values:\")\n",
    "result = {}\n",
    "risk = []\n",
    "for i, var in enumerate(target_variables):\n",
    "    value = predictions[0][i]\n",
    "    unit = ''\n",
    "    if var == 'Rain':\n",
    "        unit = 'mm'\n",
    "    elif var == 'Solar Exposure':\n",
    "        unit = 'kWh/m^2'\n",
    "    elif var == 'Temperature':\n",
    "        unit = '째C'\n",
    "    elif var == 'Carbon Emissions':\n",
    "        unit = 'units'\n",
    "    result[var] = f\"{value} {unit}\"\n",
    "\n",
    "    # Calculate risks based on thresholds\n",
    "    # Rain\n",
    "    if var == 'Rain':\n",
    "        if (40 < float(result[var].split()[0]) <= 50):\n",
    "            risk.append('Low Risk')\n",
    "        elif (35 < float(result[var].split()[0]) <= 40 or 50 < float(result[var].split()[0]) <= 55):\n",
    "            risk.append('Medium Risk')\n",
    "        else:\n",
    "            risk.append('High Risk')\n",
    "\n",
    "    # Solar Exposure\n",
    "    elif var == 'Solar Exposure':\n",
    "        if (14 < float(result[var].split()[0]) <= 16):\n",
    "            risk.append('Low Risk')\n",
    "        elif (10 < float(result[var].split()[0]) <= 14 or 16 < float(result[var].split()[0]) <= 20):\n",
    "            risk.append('Medium Risk')\n",
    "        else:\n",
    "            risk.append('High Risk')\n",
    "\n",
    "    # Temperature\n",
    "    elif var == 'Temperature':\n",
    "        if (18 < float(result[var].split()[0]) <= 24):\n",
    "            risk.append('Low Risk')\n",
    "        elif (10 < float(result[var].split()[0]) <= 18 or 24 < float(result[var].split()[0]) <= 30):\n",
    "            risk.append('Medium Risk')\n",
    "        else:\n",
    "            risk.append('High Risk')\n",
    "\n",
    "    # Carbon Emissions\n",
    "    elif var == 'Carbon Emissions':\n",
    "        if (0 < float(result[var].split()[0]) <= 90000):\n",
    "            risk.append('Low Risk')\n",
    "        elif (90000 < float(result[var].split()[0]) <= 100000):\n",
    "            risk.append('Medium Risk')\n",
    "        else:\n",
    "            risk.append('High Risk')\n",
    "\n",
    "# Print the results and risk\n",
    "print(result)\n",
    "print('\\nRisk:\\n', risk)\n",
    "\n",
    "# Return the risk factor based on thresholds\n",
    "high_count=0\n",
    "low_count=0\n",
    "med_count=0\n",
    "\n",
    "for i in risk:\n",
    "    if i == 'High Risk':\n",
    "        high_count+=1\n",
    "    elif i == 'Medium Risk':\n",
    "        med_count+=1\n",
    "    else:\n",
    "        low_count+=1\n",
    "\n",
    "print('\\nFinal Result:')\n",
    "\n",
    "if high_count >= 2:\n",
    "    print('High Risk!!!')\n",
    "elif high_count == 1:\n",
    "    print('Medium Risk!!!')\n",
    "else:\n",
    "    print('Low Risk!!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e52c9643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 55584349860.94543\n",
      "Root Mean Squared Error: 235763.33442871354\n",
      "R-squared (R2) Score: 0.22355255252220507\n",
      "MAPE Score: 2.4979474693158322e+19\n"
     ]
    }
   ],
   "source": [
    "                    ##### WORST SCORES....... IGNORE!!!!!!!! #####\n",
    "\n",
    "y_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "import numpy as np\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared (R2) Score:\", r2)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(\"MAPE Score:\", mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8c325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Below are tested ones.. not working #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b09e87e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (2886, 4) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-bb18f1bb8f93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0msvr_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# Make predictions for an example input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m             X, y = self._validate_data(X, y, dtype=np.float64,\n\u001b[0m\u001b[0;32m    170\u001b[0m                                        \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m                                        accept_large_sparse=False)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    824\u001b[0m                         ensure_2d=False, dtype=None)\n\u001b[0;32m    825\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 826\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    827\u001b[0m         \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    828\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'O'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, warn)\u001b[0m\n\u001b[0;32m    862\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 864\u001b[1;33m     raise ValueError(\n\u001b[0m\u001b[0;32m    865\u001b[0m         \u001b[1;34m\"y should be a 1d array, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m         \"got an array of shape {} instead.\".format(shape))\n",
      "\u001b[1;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (2886, 4) instead."
     ]
    }
   ],
   "source": [
    "# # Importing necessary libraries\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.svm import SVR\n",
    "\n",
    "# # Load the dataset\n",
    "# data = pd.read_excel('final_vic_data.xlsx')\n",
    "\n",
    "# # Dropping suburb as Postcode and suburb means the same\n",
    "# data.drop(columns='suburb', inplace=True)\n",
    "# data.rename(columns={'area_ha':'Area of the land per hectares', 'Solar_exposure':'Solar Exposure', \n",
    "#                      'carbon_gross_emissions':'Carbon Emissions', 'tc_loss_ha':'Tree Loss per hectares'}, inplace=True)\n",
    "\n",
    "# # Features and target variable\n",
    "# features = ['Postcode', 'threshold', 'Area of the land per hectares', 'Year', 'Tree Loss per hectares']\n",
    "# target_variables = ['Rain', 'Solar Exposure', 'Temperature', 'Carbon Emissions']\n",
    "\n",
    "# # Split the data into features (X) and target variables (y)\n",
    "# X = data[features]\n",
    "# y = data[target_variables]\n",
    "\n",
    "# # Standardize features\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # Splitting the data into 80% training and 20% testing\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Initialize the Support Vector Regression (SVR) model\n",
    "# svr_model = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "\n",
    "# # Train the model\n",
    "# svr_model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions for an example input\n",
    "# example_input = [[3672, 75, 821143, 2023, 1]]\n",
    "# example_input_scaled = scaler.transform(example_input)\n",
    "# predictions = svr_model.predict(example_input_scaled)\n",
    "\n",
    "# # Print the predicted values\n",
    "# print(\"Predicted values:\")\n",
    "# result = {}\n",
    "# risk = []\n",
    "# for i, var in enumerate(target_variables):\n",
    "#     value = predictions[0][i]\n",
    "#     unit = ''\n",
    "#     if var == 'Rain':\n",
    "#         unit = 'mm'\n",
    "#     elif var == 'Solar Exposure':\n",
    "#         unit = 'kWh/m^2'\n",
    "#     elif var == 'Temperature':\n",
    "#         unit = '째C'\n",
    "#     elif var == 'Carbon Emissions':\n",
    "#         unit = 'units'\n",
    "#     result[var] = f\"{value} {unit}\"\n",
    "\n",
    "#     # Calculate risks based on thresholds\n",
    "#         # Rain\n",
    "#     if var == 'Rain':\n",
    "#         if (40 < float(result[var].split()[0]) <= 50):\n",
    "#             risk.append('Low Risk')\n",
    "#         elif (35 < float(result[var].split()[0]) <= 40 or 50 < float(result[var].split()[0]) <= 55):\n",
    "#             risk.append('Medium Risk')\n",
    "#         else:\n",
    "#             risk.append('High Risk')\n",
    "\n",
    "#     # Solar Exposure\n",
    "#     elif var == 'Solar Exposure':\n",
    "#         if (14 < float(result[var].split()[0]) <= 16):\n",
    "#             risk.append('Low Risk')\n",
    "#         elif (10 < float(result[var].split()[0]) <= 14 or 16 < float(result[var].split()[0]) <= 20):\n",
    "#             risk.append('Medium Risk')\n",
    "#         else:\n",
    "#             risk.append('High Risk')\n",
    "\n",
    "#     # Temperature\n",
    "#     elif var == 'Temperature':\n",
    "#         if (18 < float(result[var].split()[0]) <= 24):\n",
    "#             risk.append('Low Risk')\n",
    "#         elif (10 < float(result[var].split()[0]) <= 18 or 24 < float(result[var].split()[0]) <= 30):\n",
    "#             risk.append('Medium Risk')\n",
    "#         else:\n",
    "#             risk.append('High Risk')\n",
    "\n",
    "#     # Carbon Emissions\n",
    "#     elif var == 'Carbon Emissions':\n",
    "#         if (0 < float(result[var].split()[0]) <= 90000):\n",
    "#             risk.append('Low Risk')\n",
    "#         elif (90000 < float(result[var].split()[0]) <= 100000):\n",
    "#             risk.append('Medium Risk')\n",
    "#         else:\n",
    "#             risk.append('High Risk')\n",
    "\n",
    "# # Print the results and risk\n",
    "# print(result)\n",
    "# print('\\nRisk:\\n', risk)\n",
    "\n",
    "# # Return the risk factor based on thresholds\n",
    "# high_count=0\n",
    "# low_count=0\n",
    "# med_count=0\n",
    "\n",
    "# for i in risk:\n",
    "#     if i == 'High Risk':\n",
    "#         high_count+=1\n",
    "#     elif i == 'Medium Risk':\n",
    "#         med_count+=1\n",
    "#     else:\n",
    "#         low_count+=1\n",
    "\n",
    "# print('\\nFinal Result:')\n",
    "\n",
    "# if high_count >= 2:\n",
    "#     print('High Risk!!!')\n",
    "# elif high_count == 1:\n",
    "#     print('Medium Risk!!!')\n",
    "# else:\n",
    "#     print('Low Risk!!!')\n",
    "\n",
    "\n",
    "#                     ##### WORST SCORES....... IGNORE!!!!!!!! #####\n",
    "\n",
    "# y_pred = svr_model.predict(X_test)\n",
    "\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# import numpy as np\n",
    "# rmse = np.sqrt(mse)\n",
    "# print(\"Root Mean Squared Error:\", rmse)\n",
    "\n",
    "# from sklearn.metrics import r2_score\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "# print(\"R-squared (R2) Score:\", r2)\n",
    "\n",
    "# from sklearn.metrics import mean_absolute_percentage_error\n",
    "# mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "# print(\"MAPE Score:\", mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4acf345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85039ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac89c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Carbon emission\n",
    "# # Choose a target variable (e.g., 'carbon_gross_emissions')\n",
    "# target_column = 'carbon_gross_emissions'\n",
    "\n",
    "# # Separate features and target variable\n",
    "# col_todrop = ['Rain', 'Temperature', 'Solar_exposure', 'carbon_gross_emissions']\n",
    "# X = df.drop(columns=col_todrop)\n",
    "# y = df[target_column]\n",
    "\n",
    "# # Encoding and standardizing\n",
    "\n",
    "# # One-hot encode the 'suburb' column\n",
    "# X_encoded = pd.get_dummies(X, columns=['suburb'], drop_first=True)\n",
    "\n",
    "# # Standardize the numeric features\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X_encoded)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.5, random_state=42)\n",
    "# print(\"Training set shape:\")\n",
    "# print(\"X_train shape:\", X_train.shape)\n",
    "# print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "# print(\"\\nTesting set shape:\")\n",
    "# print(\"X_test shape:\", X_test.shape)\n",
    "# print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# # Fitting and predicting the model\n",
    "# clf = DecisionTreeRegressor()\n",
    "# fit = clf.fit(X_train, y_train)\n",
    "# y_pred_carbon = fit.predict(X_test)\n",
    "\n",
    "# print(\"\\nModel fitting completion success for Carbon Emission!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcb1c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# # Load your dataset here (replace 'df' with your actual DataFrame)\n",
    "\n",
    "# # Carbon emission\n",
    "# # Choose a target variable (e.g., 'carbon_gross_emissions')\n",
    "# target_column = 'carbon_gross_emissions'\n",
    "\n",
    "# # Separate features and target variable\n",
    "# col_todrop = ['Rain', 'Temperature', 'Solar_exposure', 'carbon_gross_emissions']\n",
    "# X = df.drop(columns=col_todrop)\n",
    "# # X = df.drop(columns=[target_column])\n",
    "# y = df[target_column]\n",
    "\n",
    "# # Encoding and standardizing\n",
    "\n",
    "# # One-hot encode the 'suburb' and 'Postcode' columns\n",
    "# X_encoded = pd.get_dummies(X, columns=['suburb', 'Postcode'], drop_first=True)\n",
    "\n",
    "# # Standardize the numeric features\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X_encoded)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# # Fitting and predicting the model\n",
    "# clf = DecisionTreeRegressor()\n",
    "# fit = clf.fit(X_train, y_train)\n",
    "\n",
    "# # User input for Postcode\n",
    "# user_postcode = 3377  # Replace with the user's input\n",
    "\n",
    "# # Create a feature vector for prediction\n",
    "# # Include the user's Postcode as a one-hot encoded feature\n",
    "# user_feature_vector = pd.DataFrame([[0] * len(X_encoded.columns)], columns=X_encoded.columns)\n",
    "# user_feature_vector[f'Postcode_{user_postcode}'] = 1\n",
    "\n",
    "# # Standardize the user feature vector using the same scaler\n",
    "# user_feature_vector_scaled = scaler.transform(user_feature_vector)\n",
    "\n",
    "# # Make a prediction for the user's feature vector\n",
    "# predicted_emissions = fit.predict(user_feature_vector_scaled)\n",
    "\n",
    "\n",
    "# print(f\"Predicted 'carbon_gross_emissions' for Postcode {user_postcode}: {predicted_emissions[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4edebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# # Load your dataset here (replace 'df' with your actual DataFrame)\n",
    "\n",
    "# # Carbon emission\n",
    "# # Choose a target variable (e.g., 'carbon_gross_emissions')\n",
    "# target_column = 'carbon_gross_emissions'\n",
    "\n",
    "# # Separate features and target variable\n",
    "# col_todrop = ['Rain', 'Temperature', 'Solar_exposure', 'carbon_gross_emissions']\n",
    "# X = df.drop(columns=col_todrop)\n",
    "# # X = df.drop(columns=[target_column])\n",
    "# y = df[target_column]\n",
    "\n",
    "# # Encoding and standardizing\n",
    "\n",
    "# # One-hot encode the 'suburb' column\n",
    "# X_encoded = pd.get_dummies(X, columns=['suburb'], drop_first=True)\n",
    "\n",
    "# # Standardize the numeric features\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X_encoded)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# # Fitting and predicting the model\n",
    "# clf = DecisionTreeRegressor()\n",
    "# fit = clf.fit(X_train, y_train)\n",
    "\n",
    "# # User input for Postcode and tree_loss\n",
    "# user_postcode = 3377  # Replace with the user's input for Postcode\n",
    "# user_tree_loss = 50.0  # Replace with the user's input for tree_loss\n",
    "\n",
    "# # Create a feature vector for prediction\n",
    "# # Include the user's Postcode as a one-hot encoded feature\n",
    "# user_feature_vector = pd.DataFrame([[0] * len(X_encoded.columns)], columns=X_encoded.columns)\n",
    "\n",
    "# # Set the values for the user's Postcode and tree_loss\n",
    "# user_feature_vector[f'Postcode_{user_postcode}'] = 1\n",
    "# user_feature_vector['tree_loss'] = user_tree_loss\n",
    "\n",
    "# # Make sure the number of features in user_feature_vector matches the number in X_encoded\n",
    "# # If X_encoded has additional columns not in user_feature_vector, add them as zeros\n",
    "# missing_columns = set(X_encoded.columns) - set(user_feature_vector.columns)\n",
    "# for col in missing_columns:\n",
    "#     user_feature_vector[col] = 0\n",
    "\n",
    "# # Standardize the user feature vector using the same scaler\n",
    "# user_feature_vector_scaled = scaler.transform(user_feature_vector)\n",
    "\n",
    "# # Make a prediction for the user's feature vector\n",
    "# predicted_emissions = fit.predict(user_feature_vector_scaled)\n",
    "\n",
    "# print(f\"Predicted 'carbon_gross_emissions' for Postcode {user_postcode} and tree_loss {user_tree_loss}: {predicted_emissions[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480fa28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# # Load your data into a DataFrame (assuming you have a CSV file)\n",
    "# data = pd.read_excel('final_vic_data.xlsx')\n",
    "\n",
    "# # Perform one-hot encoding for categorical variables\n",
    "# data = pd.get_dummies(data, columns=['suburb', 'Postcode'])\n",
    "\n",
    "# # Define your features (X) and target variable (y)\n",
    "# X = data[['threshold', 'area_ha', 'Year', 'tc_loss_ha']]\n",
    "# y_rain = data['Rain']\n",
    "# y_solar_exposure = data['Solar_exposure']\n",
    "# y_temperature = data['Temperature']\n",
    "# y_carbon_gross_emissions = data['carbon_gross_emissions']\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_rain_train, y_rain_test = train_test_split(X, y_rain, test_size=0.2, random_state=42)\n",
    "# X_train, X_test, y_solar_exposure_train, y_solar_exposure_test = train_test_split(X, y_solar_exposure, test_size=0.2, random_state=42)\n",
    "# X_train, X_test, y_temperature_train, y_temperature_test = train_test_split(X, y_temperature, test_size=0.2, random_state=42)\n",
    "# X_train, X_test, y_carbon_emissions_train, y_carbon_emissions_test = train_test_split(X, y_carbon_gross_emissions, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create separate Random Forest models for each target variable\n",
    "# rf_rain = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# rf_solar_exposure = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# rf_temperature = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# rf_carbon_emissions = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# # Fit the models to the training data\n",
    "# rf_rain.fit(X_train, y_rain_train)\n",
    "# rf_solar_exposure.fit(X_train, y_solar_exposure_train)\n",
    "# rf_temperature.fit(X_train, y_temperature_train)\n",
    "# rf_carbon_emissions.fit(X_train, y_carbon_emissions_train)\n",
    "\n",
    "# # Make predictions on the test data\n",
    "# y_rain_pred = rf_rain.predict(X_test)\n",
    "# y_solar_exposure_pred = rf_solar_exposure.predict(X_test)\n",
    "# y_temperature_pred = rf_temperature.predict(X_test)\n",
    "# y_carbon_emissions_pred = rf_carbon_emissions.predict(X_test)\n",
    "\n",
    "# # Evaluate the models\n",
    "# print(\"Rain - Mean Squared Error:\", mean_squared_error(y_rain_test, y_rain_pred))\n",
    "# print(\"Solar Exposure - Mean Squared Error:\", mean_squared_error(y_solar_exposure_test, y_solar_exposure_pred))\n",
    "# print(\"Temperature - Mean Squared Error:\", mean_squared_error(y_temperature_test, y_temperature_pred))\n",
    "# print(\"Carbon Gross Emissions - Mean Squared Error:\", mean_squared_error(y_carbon_emissions_test, y_carbon_emissions_pred))\n",
    "\n",
    "# print(\"Rain - R-squared:\", r2_score(y_rain_test, y_rain_pred))\n",
    "# print(\"Solar Exposure - R-squared:\", r2_score(y_solar_exposure_test, y_solar_exposure_pred))\n",
    "# print(\"Temperature - R-squared:\", r2_score(y_temperature_test, y_temperature_pred))\n",
    "# print(\"Carbon Gross Emissions - R-squared:\", r2_score(y_carbon_emissions_test, y_carbon_emissions_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650dac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flask import Flask, render_template, request\n",
    "# import pandas as pd\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# import joblib\n",
    "\n",
    "# # Load your dataset\n",
    "# df = pd.read_excel('final_vic_data.xlsx')\n",
    "\n",
    "# # Choose a target variable\n",
    "# # target_column = 'carbon_emission'\n",
    "# #target_column = 'carbon_gross_emissions'\n",
    "# # Define the target variables\n",
    "# target_columns = ['Rain', 'Temperature', 'Solar_exposure', 'carbon_gross_emissions']\n",
    "\n",
    "# # # Separate features and target variables\n",
    "# # X = df.drop[['suburb', 'postcode', 'tree_loss']]\n",
    "# # y = df[target_columns]\n",
    "\n",
    "# # # Create a one-hot encoding for 'suburb' (optional, depending on your dataset)\n",
    "# # X_encoded = pd.get_dummies(X, columns=['suburb'], drop_first=True)\n",
    "\n",
    "\n",
    "# # # Separate features and target variable\n",
    "# # col_todrop = ['Rain', 'Temperature', 'Solar_exposure', 'carbon_gross_emissions']\n",
    "\n",
    "# # Separate features and target variable\n",
    "# X = df.drop(columns=target_columns)\n",
    "# y = df[target_columns]\n",
    "\n",
    "# # Create a one-hot encoding for 'suburb' (optional, depending on your dataset)\n",
    "\n",
    "# X_encoded = pd.get_dummies(X, columns=['suburb'], drop_first=True)\n",
    "\n",
    "# # Train your Decision Tree Regressor model\n",
    "# clf = DecisionTreeRegressor()\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# # Save the trained model to a file\n",
    "# joblib.dump(clf, 'model.pkl')\n",
    "\n",
    "\n",
    "# # Initialize the Flask app\n",
    "# app = Flask(__name__)\n",
    "\n",
    "# # Load your trained Decision Tree Regressor model\n",
    "# # Replace 'model.pkl' with the actual file path to your trained model\n",
    "# model = joblib.load('model.pkl')\n",
    "\n",
    "# @app.route('/', methods=['GET', 'POST'])\n",
    "# def predict():\n",
    "#     if request.method == 'POST':\n",
    "#         # Get user input from the form\n",
    "#         user_postcode = int(request.form['Postcode'])\n",
    "#         user_tree_loss = float(request.form['tc_loss_ha'])\n",
    "\n",
    "#         # Create a feature vector for prediction\n",
    "#         user_feature_vector = pd.DataFrame({'Postcode': [user_postcode], 'tc_loss_ha': [user_tree_loss]})\n",
    "\n",
    "#         # Use one-hot encoding if 'suburb' is categorical\n",
    "#         user_feature_vector_encoded = pd.get_dummies(user_feature_vector, columns=['suburb'], drop_first=True)\n",
    "\n",
    "#         # Make predictions using the model\n",
    "#         predicted_features = model.predict(user_feature_vector_encoded)\n",
    "\n",
    "#         # Display the predicted features\n",
    "#         return render_template('result.html', predicted_features=predicted_features)\n",
    "\n",
    "#     return render_template('index.html')\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8279ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load your dataset here (replace 'df' with your actual DataFrame)\n",
    "# df = pd.read_csv('your_dataset.csv')  # Replace 'your_dataset.csv' with the actual file path\n",
    "\n",
    "# # Define the target variables\n",
    "# target_columns = ['Rain', 'Temperature', 'Solar_exposure', 'carbon_gross_emissions']\n",
    "\n",
    "# # Separate features and target variables\n",
    "# X = df[['suburb', 'postcode', 'tree_loss']]\n",
    "# y = df[target_columns]\n",
    "\n",
    "# # Create a one-hot encoding for 'suburb' (optional, depending on your dataset)\n",
    "# X_encoded = pd.get_dummies(X, columns=['suburb'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bc161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rain\n",
    "# # Choose a target variable (e.g., 'Rain')\n",
    "# target_column = 'Rain'\n",
    "\n",
    "# # Separate features and target variable\n",
    "# X = df.drop(columns=[target_column])\n",
    "# y = df[target_column]\n",
    "\n",
    "# # Encoding and standardizing\n",
    "\n",
    "# # One-hot encode the 'suburb' column\n",
    "# X_encoded = pd.get_dummies(X, columns=['suburb'], drop_first=True)\n",
    "\n",
    "# # Standardize the numeric features\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X_encoded)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.5, random_state=42)\n",
    "# print(\"Training set shape:\")\n",
    "# print(\"X_train shape:\", X_train.shape)\n",
    "# print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "# print(\"\\nTesting set shape:\")\n",
    "# print(\"X_test shape:\", X_test.shape)\n",
    "# print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# # Fitting and predicting the model\n",
    "# regressor = DecisionTreeRegressor() \n",
    "# fit = regressor.fit(X_train, y_train)\n",
    "# y_pred_rain = fit.predict(X_test)\n",
    "\n",
    "# print(\"\\nModel fitting completion success for Rain!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bb0116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Solar Exposure\n",
    "# # Choose a target variable (e.g., 'Solar_exposure')\n",
    "# target_column = 'Solar_exposure'\n",
    "\n",
    "# # Separate features and target variable\n",
    "# X = df.drop(columns=[target_column])\n",
    "# y = df[target_column]\n",
    "\n",
    "# # Encoding and standardizing\n",
    "\n",
    "# # One-hot encode the 'suburb' column\n",
    "# X_encoded = pd.get_dummies(X, columns=['suburb'], drop_first=True)\n",
    "\n",
    "# # Standardize the numeric features\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X_encoded)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.5, random_state=42)\n",
    "# print(\"Training set shape:\")\n",
    "# print(\"X_train shape:\", X_train.shape)\n",
    "# print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "# print(\"\\nTesting set shape:\")\n",
    "# print(\"X_test shape:\", X_test.shape)\n",
    "# print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# # Fitting and predicting the model\n",
    "# regressor = DecisionTreeRegressor()\n",
    "# fit = regressor.fit(X_train, y_train)\n",
    "# y_pred_solar = fit.predict(X_test)\n",
    "\n",
    "# print(\"\\nModel fitting completion success for Solar_exposure!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69582d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Temperature\n",
    "# # Choose a target variable (e.g., 'Temperature')\n",
    "# target_column = 'Temperature'\n",
    "\n",
    "# # Separate features and target variable\n",
    "# X = df.drop(columns=[target_column])\n",
    "# y = df[target_column]\n",
    "\n",
    "# # Encoding and standardizing\n",
    "\n",
    "# # One-hot encode the 'suburb' column\n",
    "# X_encoded = pd.get_dummies(X, columns=['suburb'], drop_first=True)\n",
    "\n",
    "# # Standardize the numeric features\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X_encoded)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.5, random_state=42)\n",
    "# print(\"Training set shape:\")\n",
    "# print(\"X_train shape:\", X_train.shape)\n",
    "# print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "# print(\"\\nTesting set shape:\")\n",
    "# print(\"X_test shape:\", X_test.shape)\n",
    "# print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# # Fitting and predicting the model\n",
    "# regressor = DecisionTreeRegressor()\n",
    "# fit = regressor.fit(X_train, y_train)\n",
    "# y_pred_temp = fit.predict(X_test)\n",
    "\n",
    "# print(\"\\nModel fitting completion success for Temperature!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e379d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Printing the predictions\n",
    "# print('Predictions of Carbon Emission: ', y_pred_carbon)\n",
    "# print('\\nPredictions of Rain: ', y_pred_rain)\n",
    "# print('\\nPredictions of Solar Exposure: ', y_pred_solar)\n",
    "# print('\\nPredictions of Temperature: ', y_pred_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c2d75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "\n",
    "# # Define your neural network model\n",
    "# model = keras.Sequential([\n",
    "#     #layers.Input(shape=(5,)),  # Input layer with 10 features\n",
    "#    #layers.Dense(128, activation='relu'),\n",
    "#     layers.Dense(64, activation='relu'),  # Hidden layer with 64 neurons and ReLU activation\n",
    "#     #layers.Dropout(0.3),  # Dropout layer\n",
    "#     layers.Dense(32, activation='relu'),  # Hidden layer with 32 neurons and ReLU activation\n",
    "#     layers.Dense(1, activation='linear')  # Output layer for regression with linear activation\n",
    "# ])\n",
    "\n",
    "# # Compile the model with mean squared error as the loss and Adam optimizer\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "# model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "# # Define the number of epochs and batch size\n",
    "# epochs = 10\n",
    "# batch_size = 32\n",
    "\n",
    "# # Train the model with your data (X_train, y_train)\n",
    "# history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# test_loss = model.evaluate(X_test, y_test)\n",
    "# print(f'Test loss: {test_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff10a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# # Load your data into a DataFrame (assuming you have a CSV file)\n",
    "# data = pd.read_excel('final_vic_data.xlsx')\n",
    "\n",
    "# # Perform one-hot encoding for categorical variables\n",
    "# data = pd.get_dummies(data, columns=['suburb', 'Postcode'])\n",
    "\n",
    "# # Define your features (X) and target variable (y)\n",
    "# X = data[['threshold', 'area_ha', 'Year', 'tc_loss_ha']]\n",
    "# y_rain = data['Rain']\n",
    "# y_solar_exposure = data['Solar_exposure']\n",
    "# y_temperature = data['Temperature']\n",
    "# y_carbon_gross_emissions = data['carbon_gross_emissions']\n",
    "\n",
    "# # Create separate Random Forest models for each target variable\n",
    "# rf_rain = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# rf_solar_exposure = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# rf_temperature = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# rf_carbon_emissions = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# # Fit the models to the entire dataset\n",
    "# rf_rain.fit(X, y_rain)\n",
    "# rf_solar_exposure.fit(X, y_solar_exposure)\n",
    "# rf_temperature.fit(X, y_temperature)\n",
    "# rf_carbon_emissions.fit(X, y_carbon_gross_emissions)\n",
    "\n",
    "# # Collect user input for 'Postcode' and 'tc_loss_ha'\n",
    "# user_postcode = 3377  # Replace with the user's input for Postcode\n",
    "# user_tc_loss_ha = 100  # Replace with the user's input for tc_loss_ha\n",
    "\n",
    "# # Prepare the user input data\n",
    "# user_input = pd.DataFrame({\n",
    "#     'threshold': [50],  # Example threshold value, replace with the user's input\n",
    "#     'area_ha': [421143],  # Example area_ha value, replace with the user's input\n",
    "#     'Year': [2003],  # Example Year value, replace with the user's input\n",
    "#     'tc_loss_ha': [user_tc_loss_ha],  # User's input for tc_loss_ha\n",
    "#     f'Postcode_{user_postcode}': [user_postcode]  # One-hot encoding for user's Postcode\n",
    "# })\n",
    "\n",
    "# # Make predictions using the models\n",
    "# rain_prediction = rf_rain.predict(user_input)[0]\n",
    "# solar_exposure_prediction = rf_solar_exposure.predict(user_input)[0]\n",
    "# temperature_prediction = rf_temperature.predict(user_input)[0]\n",
    "# carbon_emissions_prediction = rf_carbon_emissions.predict(user_input)[0]\n",
    "\n",
    "# print(\"Predicted Rain:\", rain_prediction)\n",
    "# print(\"Predicted Solar Exposure:\", solar_exposure_prediction)\n",
    "# print(\"Predicted Temperature:\", temperature_prediction)\n",
    "# print(\"Predicted Carbon Gross Emissions:\", carbon_emissions_prediction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
